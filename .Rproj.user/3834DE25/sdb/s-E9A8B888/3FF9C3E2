{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Applied Bayesian Statistics - Assignment 2\"\nauthor: \"Benjamin Cole - s3412349\"\ndate: \"12 September 2018\"\noutput: html_document\n---\n\n### Data and Packages\n\n```{r, echo=T, message=F, warning=F, results='hide'}\n\npackages <- c(\"tidyverse\",\n              \"ggplot2\",\n              \"rmarkdown\",\n              \"knitr\",\n              \"kableExtra\",\n              \"purrr\",\n              \"scales\",\n              \"rjags\",\n              \"runjags\",\n              \"coda\",\n              \"readr\",\n              \"beepr\",\n              \"grid\",\n              \"gridExtra\")\n\nlapply(packages, library, character.only=T)\n\nsource(\"DBDA2E-utilities.R\")\n\n# Data\nPropertyPrices <- read_csv(\"Assignment2PropertyPrices.csv\")\n\nPropertyPrices$SalePrice <- as.numeric(PropertyPrices$SalePrice)\n\n```\n\n# **Part A**\n\n### Model Diagram Parameters\n\n```{r, fig.width=10, fig.height=7, warning=F, message=F, echo=T}\n\nPropertyPrices %>% summarise(\"Mean Sale Price\" = mean(SalePrice),\n                             \"Sale Price Var\" = var(SalePrice),\n                             \"Sale Price SD\" = sd(SalePrice),\n                             \"Min Sale Price\" = min(SalePrice),\n                             \"Max Sale Price\" = max(SalePrice),\n                             \"n Observations\" = n()) %>% \n  gather(key=\"Measure\",\n         value=\"Value\") %>% \n  kable(align=\"lr\", \"html\",\n        format.args = list(trim=F,\n                           digits=1,\n                           nsmall=1,\n                           scientific=F,\n                           big.mark=\" \")) %>% \n  kable_styling(full_width=F,\n                bootstrap_options=)\n\n# Sale price hist\nggplot(PropertyPrices, aes(x=SalePrice)) +\n  geom_histogram(aes(fill=factor(PropertyPrices$PropertyType,\n                                    labels=c(\"House\", \"Unit\"))),\n                 col=\"white\",\n                 bins=40,\n                 alpha=2/3) +\n  scale_x_continuous(\"Sale Price (AUD)\",\n                     labels=dollar) +\n  scale_y_continuous(\"Number of Properties\",\n                     label=comma) + \n  scale_fill_manual(\"Property Type\",\n                    values=c(\"blue3\", \"cyan3\")) +\n  ggtitle(\"Sale Price Histogram\") +\n  theme_minimal()\n\n```\n\n`SalePrice` is right-skewed and not normally distributed. However, as n=10,000, CLT can be assumed and that distribution of sampling means would be Gaussian. As such, the Bayesian model diagram is:\n\n![Model Diagram](Model Diagram Part A.png)\n\n### JAGS Data and Model blocks\n\n```{r, warning=F, message=F, echo=T}\n\nvectSalePrice <- PropertyPrices$SalePrice\nnTotal <- length(vectSalePrice)\n\ndataList <- list(\n  vectSalePrice = vectSalePrice,\n  nTotal = nTotal\n)\n\nvar(vectSalePrice)\n\n\nmodelString <- c(\"\nmodel {\n  # priors\n  mu ~ dnorm(609360.2, tau)\n  tau ~ dgamma(0.001, 1/(262475685684))\n\n  # likelihood\n  for (i in 1:nTotal) {\n    vectSalePrice[i] ~ dnorm(mu, tau)\n  }\n}\n\"\n)\n\n\nwriteLines(modelString, con=\"normJAGSmodel.txt\")\n\n```\n\n### Compile Model and run Markov Chains\n\n\n```{r, warning=F, message=F, echo=T, fig.width=10, fig.height=8}\n\njagsModel <- jags.model(file=\"normJAGSmodel.txt\",\n                        data=dataList,\n                        n.chains=5,\n                        n.adapt=150\n                        )\n\nupdate(jagsModel,\n       n.iter=500)\n\ncodaSamples <- coda.samples(jagsModel,\n                            variable.names=c(\"mu\"),\n                            n.iter=2500\n                            )\n\n# beep(2)\n\n# Display MCMC diagnostics\ndiagMCMC(codaObject=codaSamples,\n         parName=\"mu\")\n\n# Display the posterior distribution of mu\nplotPost(codaSamples[,\"mu\"],\n          main=\"mu\",\n          xlab=bquote(mu)\n         )\n\n```\n\n### Interpretation\n\n#### Chain Representativeness\n\nThe chains all seem to have settled about \\bar{x}\\approx 610\\ 000. The chains seem to mix well, with most values of each iteration being between 590\\ 000 and 630\\ 000. After the burn-in period, the chains can be seen in the smoothed density plot to overlap well. The shrink factor can also be observed to be very close to one after the burn-in period. The 95\\% High Density Interval is (598\\ 981.1, 618\\ 939.5), with a mode of 609 000.\n\n#### Chain Accuracy\n\nThe *Effective Sample Size* is the same length of the chain (ESS=12\\ 500) and the *Monte Carlo standard error* (MCSE\\approx46.2) is somewhat low compared to \\bar{x}\\approx 609\\ 000 and the ESS. Considering the *Effective Sample Size* adn the *Monte Carlo standard error*, it can be suggested that the chains are accurate represenations of the posterior probability distribution.\n\n\n# **Part B**\n\n### Regression Model\n\nThe following expert knowledge has been provided for the predictor variables of the data set, which allows for prior distributions to be defined:\n\"\n* Area: Every m2 increase in land size increases the sales price by 90 AUD. This is a very strong expert knowledge.\n* Bedrooms: Every additional bedroom increases the sales price by 100,000AUD. This is a weak expert knowledge.\n* Bathrooms: There is no expert knowledge on the number of bathrooms.\n* CarParks: Every additional car space increases the sales price by 120,000AUD. This is a strong expert knowledge.\n* PropertyType: If the property is a unit, the sale price will be 150,000 AUD less than that of a house on the average. This is a very strong expert knowledge.\n\"\n\nThese prior distributions will be used in the multiple linear regression equation\n\\mu= \\beta_0 + \\beta_j X_j\nOver five predictor variables, this equation can be specified as:\n\\mu=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5, which can be modelled as:\n\n![Generic Model Diagram](Model Diagram Part B.png)\n\nWith the distributions of predictors (denoted \\boldsymbol{j} in the model diagram above) defined per predictor as:\n\n1. Area:\n\\beta_1 = j\\sim N\\leftmu=90, \\tau=\\leftfrac{1}{\\sigma_y^2}\\right)\\times0.01\\right)\n\n2. Bedrooms:\n\\beta_2 =j\\sim N\\leftmu=100\\ 000,\\tau=\\leftfrac{1}{\\sigma_y^2} \\right)\\times0.75 \\right)\n\n3. Bathrooms\n\\beta_3 =j\\sim N\\leftmu=609\\ 360.2,\\tau=\\leftfrac{1}{\\sigma_y^2}\\right) \\times100 \\right)\n```{r, collapse=T}\n# mean of SalePrice\nmean(PropertyPrices$SalePrice) %>% format(big.mark=\" \")\n```\n\n4. CarParks:\n\\beta_4=j\\sim N\\leftmu=120\\,000,\\tau=\\leftfrac{1}{\\sigma_y^2}\\right) \\times0.1\\right)\n\n5. PropertyType:\n\\beta_5 = j\\sim N\\leftmu=150\\ 000, \\tau=\\leftfrac{1}{\\sigma_y^2}\\right)\\times 0.01\\right)\n\n#### Data and Model Block\n\n```{r}\n\nmodelString = \"\n  # Standardize the data:\n  data {\n    ym <- mean(y)\n    ysd <- sd(y)\n    ymin <- min(y)\n    for ( i in 1:Ntotal ) {\n      zy[i] <- ( y[i] - ym ) / ysd\n    }\n    for ( j in 1:Nx ) {\n      xm[j]  <- mean(x[,j])\n      xsd[j] <-   sd(x[,j])\n      for ( i in 1:Ntotal ) {\n        zx[i,j] <- ( x[i,j] - xm[j] ) / xsd[j]\n      }\n    }\n\n    # Specify the priors for original beta parameters\n    # Prior locations to reflect the expert information\n    mu0 <- ymin/2 # A dwelling with no area, no bedrooms, no bathrooms, no car parks, and is neither property type conceptually does not exist\n    mu[1] <- 90 # Area\n    mu[2] <- 100000 # Bedrooms\n    mu[3] <- ym # Bathrooms, mean of y\n    mu[4] <- 120000 # CarParks\n    mu[5] <- -150000 # PropertyType\n\n    # Prior variances to reflect the expert information    \n    Var0   <- (ysd^2)\n    Var[1] <- ((ysd)*0.1) # Area\n    Var[2] <- ((ysd)*0.75) # Bedrooms\n    Var[3] <- ((ysd)*5) # Bathrooms\n    Var[4] <- ((ysd)*0.25) # CarParks\n    Var[5] <- ((ysd)*0.1) # PropertyType\n\n    # Compute corresponding prior means and variances for the standardised parameters\n    muZ[1:Nx] <-  mu[1:Nx] * xsd[1:Nx] / ysd\n\n    muZ0 <- (mu0 + sum( mu[1:Nx] * xm[1:Nx] / xsd[1:Nx] )*ysd - ym) / ysd^2 \n\n    # Compute corresponding prior variances and variances for the standardised parameters\n    VarZ[1:Nx] <- Var[1:Nx] * ( xsd[1:Nx]/ ysd )^2\n    VarZ0 <- ((Var0)*75000) / (ysd^2)\n\n  }\n  # Specify the model for standardized data:\n  model {\n    for ( i in 1:Ntotal ) {\n      zy[i] ~ dt( zbeta0 + sum( zbeta[1:Nx] * zx[i,1:Nx] ) , 1/zsigma^2 , nu )\n    }\n\n    # Priors vague on standardized scale:\n    zbeta0 ~ dnorm( muZ0 , 1/VarZ0 )  \n    for ( j in 1:Nx ) {\n      zbeta[j] ~ dnorm( muZ[j] , 0.01/VarZ[j] )\n    }\n    zsigma ~ dgamma(0.001,0.001) #dunif( 1.0E-5 , 1.0E+1 )\n    nu ~ dexp(1/30.0)\n\n    # Transform to original scale:\n    beta[1:Nx] <- ( zbeta[1:Nx] / xsd[1:Nx] )*ysd\n    beta0 <- zbeta0*ysd  + ym - sum( zbeta[1:Nx] * xm[1:Nx] / xsd[1:Nx] )*ysd\n    sigma <- zsigma*ysd\n\n    # Compute predictions at every step of the MCMC\n    Prediction1 <- beta0 + (beta[1] * Pred1[1]) + (beta[2] * Pred1[2]) + (beta[3] * Pred1[3]) + (beta[4] * Pred1[4])\n            + (beta[5] * Pred1[5])\n\n    Prediction2 <- beta0 + (beta[1] * Pred2[1]) + (beta[2] * Pred2[2]) + (beta[3] * Pred2[3]) + (beta[4] * Pred2[4])\n            + (beta[5] * Pred2[5])\n\n    Prediction3 <- beta0 + (beta[1] * Pred3[1]) + (beta[2] * Pred3[2]) + (beta[3] * Pred3[3]) + (beta[4] * Pred3[4])\n            + (beta[5] * Pred3[5])\n\n    Prediction4 <- beta0 + (beta[1] * Pred4[1]) + (beta[2] * Pred4[2]) + (beta[3] * Pred4[3]) + (beta[4] * Pred4[4])\n            + (beta[5] * Pred4[5])\n\n    Prediction5 <- beta0 + (beta[1] * Pred5[1]) + (beta[2] * Pred5[2]) + (beta[3] * Pred5[3]) + (beta[4] * Pred5[4])\n            +( beta[5] * Pred5[5])\n  }\n\"\n\n```\n\n\n### Standardising the Data\n\nTo reduce the autocorrelation of the regression model parameters, particularly \\beta_0 & \\beta_1, the data needs to be standardised as follows: \n\nz_{\\hat{y}}=\\zeta_0 SD_y +M_y -SD_y \\leftsum_{j=1} \\zeta_j \\frac{M_{x_{j}}}{SD_{x_{j}}} + \\sum_j \\leftzeta{_j} \\frac{SD_y}{SD_{x_{j}}}\\right) \\right)\n\nThe model block above uses this method to standardise the data for \\beta_0 and each \\beta_j predictor.\n\n<br /> <br />\n\n### Assessing the Chains\n\n![Diagnostics for \\beta_0](diagbeta0.png)\n<br />\nAutocorrelation for \\beta_0 is very low, and has not resulted in an *Effective Sample Size* (ESS\\approx9\\ 800) that is almost the full length of the chain (10 000 iterations). The shrink factor approaches 1.0 and shows the chains converge well for \\beta_0. The smoothed density plots show the chains overlap well, with MCSE being comparatively low in the context of the point estimate. The 95% HDI is between \\{325\\ 000,\\ 355\\ 000\\}.\nThis estimate doesn't quite seem correct as you'd expect a property with 0 land Area, 0 Bedrooms, 0 Bathrooms, and 0 CarParks to sell for a value much closer to 0.\n\n![Diagnostics for \\beta_1](diagbeta1.png)\n<br />\n\nDespite the very strong prior knowledge stating that for every square metre increase in a plot of land equates to a \\$90 increase in Sale Price, the chains all hover around \\bar{x}\\approx 70. This indicates that even though the prior distribution was centred at \\mu=90 - with quite a small variance (or, conversely, a high precision tau)) implying high certainty of the estimate - that the sampling likelihood distribution was able to override this estimate and reduce its value for \\beta_1 in the posterior distribution from 90 to 70. As the HDI \\{62, 77\\} is not centred around zero, the estimate can be retained in the model.\n\nThe *Effective Sample Size* (ESS\\approx9500) is close to the full length of the chain (10 000 iterations) for \\beta_1. The shrink factor for \\beta_1 approaches 1.0, and the smoothed density plots of the chains can be seen to overlap well. As such, the chains can be asserted to have mixed well and are representative of an appropriately modelled posterior distribution.\n\n![Diagnostics for \\beta_2](diagbeta2.png)\n<br />\n\nWeak expert information was provided for \\beta_2, which was that Sale Price increases by \\$100 000 for each increase in the number of bedrooms. Despite this prior distribution being allocated a weak certainty, the estimate for \\beta_2 still appears to be centred around \\$-20 000, with a HDI of \\{-25\\ 000, 15\\ 000\\}.\n\nThe *Effective Sample Size* (ESS\\approx 6\\ 400) for \\beta_2 is considerably shorter than the full length of the chain, indicating that it oculd benefit from further thinning. However, The shrink factor is below 1.2 for every iteration in the chain after the burn-in period and approaches 1.0. The smoothed density plots appear to overlap well, and so the chains can be said to be moderately appropriate representations of the posterior distribution for \\beta_2.\n\n![Diagnostics for \\beta_3](diagbeta3.png)\n<br />\n**No expert knowledge** was provided for \\beta_3. As such, values from the likelihood distribution \\mu_{SalePrice}=609\\ 360.2 and \\sigma_y were assigned to \\bar{x} and \\sigma_3 (respectively) in the model for estimating the prior distribution of \\beta_3. The posterior distribution for the estimate of \\beta_3 seems to have settled around \\mu=55\\ 000, with a HDI of \\{55\\ 000, 65\\ 000\\}.\n\nThe *Effective Sample Size* (ESS \\approx 7\\ 500) is shorter than the full length of the chain (10 000 iterations), and appears it could benefit by an increased amount of thinning. The shrink factor for \\beta_3 is well below 1.2 for all iterations after the burn-in stage. The smoothed density plot shows that all the chains mix well. As such, the chains can be asserted to be moderately representative of the posterior distribution.\n\n![Diagnostics for \\beta_4](diagbeta4.png)\n<br />\n\nStrong expert knowledge was provided for \\beta_4, which was that every increase in Car Parks would result in a \\$120 000 increase in Sale Price. It appears that the chains have all settled around a \\mu=45\\ 000, with a HDI of \\{39\\ 000, 49\\ 000\\}.\n\nThe *Effective Sample Size* (ESS\\approx 8750) for \\beta_4 is also somewhat close to the full length of the chain (10 000 iterations), but could still benefit by a little extra thinning. The shrink factor is well below 1.2 for all iterations of the chain after the burn-in period and approaches 1.0. The smoothed density plot shows that the chains overlap, which indicates they have mixed well. As such, it can be asserted that the chains are moderately representative of the posterior distribution.\n\n![Diagnostics for \\beta_5](diagbeta5.png)\n<br />\nThe very strong prior knowledge provided for \\beta_5 was that if a property is a unit, it will sell for \\$150 000 less on average as compared to houses. As houses were encoded as 0 and units were encoded as 1 in the data set, -\\$150 000 was assigned to the estimate of centrality for the prior distribution. The strong prior knowledge was reflected in the posterior distribution, which was also centred around -120\\ 000 with a HDI of \\{-122\\ 500, -116\\ 000\\}.\n\nThe *Effective Sample Size* (ESS\\approx 8\\ 500) was shorter than the full length of the MCMC chain (10 000 iterations), and would benefit well from more thinning. The shrink factor was below 1.2 for all iterations of the chain after the burn-in period and approached 1.0. The smoothed density plot showed that the chains overlapped and indicated that they mixed well. As such, it can be said that the MCMC chains were suitably representative of the posterior distribution.\n\n\n### Assessing the Posterior Distributions\n<br />\n![Posterior Distribution Curves](Curves.png)\n<br />\n\n#### Posterior Distributions - \\beta_j estimates\nMost of the sentiments about the HDI stated for \\beta_j estimates above are carried through when observing the posterior distribution curves. Modes are also supplied in the plot above as follows: <br />\n**MODES** <br />\n- \\beta_0=338\\ 000 <br />\n- \\beta_1=68.9 <br />\n- \\beta_2=-20\\ 800 <br />\n- \\beta_3=54\\ 900 <br />\n- \\beta_4=44\\ 000 <br />\n- \\beta_5=-118\\ 000 <br />\n\nThe mode of the HDI for \\beta_2 is the only point estimate that varies largely from that provided by the prior knowledge. This indicates that, despite the prior knowledge of \\beta_1 having high certainty, the likelihood distribution dominated the prior and has pulled the mode of the posterior distribution away the point estimate provided in the prior knowledge. However, as the posterior distribution for the estimate of \\beta_2 does not overlap 0, it cannot be removed from the model.\n\n#### R^2 Posterior Distribution - Coefficient of Determination\nThe coefficient of determination for the multiple regression model had a narrow HDI of {0.0424, 0.0484}, centred around a mode of 0.0458. This posterior distribution of the estimate for R^2 is almost 0, which suggests no correlation nor regression. This could be due to the influence of the \\beta_2 estimate for Bedrooms being a negative HDI range not overlapping 0.\n<br /> <br />\n\n#### Posterior Distributions - Predictions\nDespite R^2 being almost non-existant, the predictions of each of the property prices were surprisingly narrow. Prediction 1 was centred around a mode of \\$374 000, Prediction 2 around a mode of \\$474 000, Prediction 3 around \\$500 000, Prediction 4 around \\$804 000, and Prediction 5 around \\$329 000.\n\n## Conclusion\nDespite all of the estimates for the \\beta_0 and \\beta_j predictors having moderate to high certainty, the coefficient of determination for the model was extremely small. It would be suggested to run the model again, but having removed \\beta_2 Bedrooms from the model. Further thinning might also improve the appropriateness of the MCMC chains and certainty of the estimates.\n\n",
    "created" : 1538436720007.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1457332471",
    "id" : "3FF9C3E2",
    "lastKnownWriteTime" : 1538315150,
    "last_content_update" : 1538315150,
    "path" : "~/RMIT/2018 RMIT SEM 2/Applied Bayesian Statistics/Bayesian.Assn.2/Assignment 2/Assignment_2.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}